main.logdir = 'logs'
logging.mute = ['matplotlib', 'PIL']

planet.num_runs = 1000
planet.ping_every = 0
planet.resume_runs = False
planet.config = 'default'
planet.params = {
    'tasks': ['habitat'],
    'isolate_envs': 'none',  # Don't change
    # 'action_repeat': 1,
    # 'num_seed_episodes': 1,
    # 'train_steps': 10,
    # 'test_steps': 10,
    'max_steps': 5e7,  # Maximum number of training steps
    'max_task_length': 500,  # Maximum sequence length
    # 'train_collects': [dict(after=10, every=10)],
    # 'test_collects': [dict(after=10, every=10)],
    # 'model_size': 10,
    # 'state_size': 5,
    # 'num_layers': 1,
    # 'num_units': 10,
    # 'batch_shape': [5, 10],
    # 'loader_every': 5,
    # 'loader_window': 2,
    # 'planner_amount': 5,
    # 'planner_topk': 2,
    # 'planner_iterations': 2,
}
planet.tf.options = {
    'log_device_placement': True,
    # 'device_count': {'GPU': 0, 'CPU': 4},
    'inter_op_parallelism_threads': 1,  # Don't change
}
planet.tf.gpu_options = {
    'allow_growth': True,
    'visible_device_list': '1',
    # 'per_process_gpu_memory_fraction': 0.5,
}

DiscreteWrapper.sample = False
Habitat.task = 'pointnav'
Habitat.dataset = 'habitat_test'
Habitat.reward_measure = 'spl'
Habitat.image_key = 'rgb'
Habitat.gpu_id = 0
