main.logdir = 'logs'
logging.mute = ['matplotlib', 'PIL']

planet.num_runs = 1
planet.ping_every = 0
planet.resume_runs = False
planet.config = 'default'

planet.params.tasks = ['habitat']
planet.params.isolate_envs = 'none'  # Don't change
planet.params.action_repeat = 1
planet.params.num_seed_episodes = 5
planet.params.train_steps = 5e3
planet.params.test_steps = 50
planet.params.max_steps = 5e7  # Maximum number of training steps
planet.params.max_task_length = 500  # Maximum sequence length
planet.params.divergence_scale = 1.0

planet.tf.options.log_device_placement = False
# planet.tf.options.device_count = {'GPU': 0, 'CPU': 4}
planet.tf.options.inter_op_parallelism_threads = 1  # Don't change

planet.tf.gpu_options.allow_growth = True
planet.tf.gpu_options.visible_device_list = '1'
# planet.tf.gpu_options.per_process_gpu_memory_fraction = 0.5

planet.tf.debugger = False

DiscreteWrapper.sample = False
AutomaticStop.enable = False
Habitat.task = 'pointnav'
Habitat.dataset = 'habitat_test'
Habitat.gpu_id = 0
Habitat.image_key = 'rgb'

DenseReward.slack_reward = -0.01
DenseReward.success_reward = 10.0
DenseReward.distance_scaling = 1.0
OptimalPathLengthReward.scaling = 1.0
CollisionPenalty.scaling = 1.0
ObstacleDistancePenalty.threshold = 0.5
ObstacleDistancePenalty.scaling = 1.0
Habitat.reward_function = @combine_rewards()
combine_rewards.rewards = [@DenseReward, @CollisionPenalty, @ObstacleDistancePenalty]
