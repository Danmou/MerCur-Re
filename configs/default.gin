main.base_logdir = 'logs'
logging.module_levels = {'matplotlib': 'WARNING', 'PIL': 'WARNING', 'imageio_ffmpeg': 'ERROR'}

### Planet ###
# Task
training.tasks = [@habitat_task()]

# Data preprocessing
numpy_episodes.train_action_noise = 0.3
numpy_episodes.success_padding = 32  # Number of padding elements equal to the last element to add after successful episodes

# Schedule
training.num_seed_episodes = 40
DataCollectionCallback.period = 1  # epochs per data collection
DataCollectionCallback.train_episodes = 4  # training episodes per data collection
DataCollectionCallback.test_episodes = 2  # episodes per test data collection
training.train_steps = 100  # training batches per epoch
training.test_steps = 10  # test steps per epoch
training.num_epochs = 1e4
CheckpointCallback.period = 5  # epochs between checkpoints
EvaluateCallback.period = 10  # epochs between evaluations
EvaluateCallback.num_episodes = 10  # episodes per evaluation
PredictionSummariesCallback.period = 5
LoggingCallback.epoch_log_period = 1
LoggingCallback.epoch_header_period = 5  # how many times to log before reprinting headers
LoggingCallback.batch_log_period = 10
LoggingCallback.batch_header_period = 10  # how many times to log before reprinting headers

# Losses
SimpleRNN.divergence_loss_free_nats = 3.0
SimpleRNN.divergence_loss_scale = 0.1
reconstruction_loss.scales = {
    'image': 600.0,
    'reward': 0.5,
    'goal': 1.0,
}

# Model complexity
Model.predictor_class = @predictors.RSSMPredictor
predictors.OpenLoopRSSMPredictor.state_size = 40
predictors.OpenLoopRSSMPredictor.belief_size = 300
predictors.OpenLoopRSSMPredictor.embed_size = 300
predictors.OpenLoopRSSMPredictor.num_layers = 1
Model.rnn_class = @rnns.SimpleRNN
Model.decoders.num_layers = 1  # used for reward and goal prediction
Model.decoders.num_units = 100  # used for reward and goal prediction
training.batch_shape = (64, 64)  # [batch_size, time_batch]

# Optimizer
get_model.optimizer = @optimizers.with_global_norm_clipping()
optimizers.with_global_norm_clipping.clip_norm = 1000.0
optimizers.with_global_norm_clipping.optimizer = @tf.keras.optimizers.Adam()
tf.keras.optimizers.Adam.learning_rate = 1e-3
tf.keras.optimizers.Adam.epsilon = 1e-4

# Planner
MPCAgent.planner = @CrossEntropyMethod
CrossEntropyMethod.horizon = 12
CrossEntropyMethod.iterations = 10
CrossEntropyMethod.amount = 1000  # number of action sequence samples per iteration
CrossEntropyMethod.top_k = 100  # number of best samples to use as basis for next iteration

# TF options
# tf.options.log_device_placement = False
# tf.options.device_count = {'GPU': 0, 'CPU': 4}
# tf.debugger = False

# GPU options
tf.gpus.memory_growth = True
tf.gpus.gpu_ids = [0]
# tf.gpu_options.per_process_gpu_memory_fraction = 0.5

### Habitat ###
# Basic
habitat_task.max_length = 150  # Maximum sequence length
habitat_task.wrappers = [@curriculum(),
                         @action_repeat()]
curriculum.enabled = False
curriculum.start_threshold = 1.5  # Initial threshold in meters.
curriculum.initial_delay = 200  # Number of episodes to wait before increasing threshold.
curriculum.increase_rate = 0.05  # Rate of increase in meters per episode.
Habitat.task = 'pointnav'
# Habitat.dataset = 'habitat_test'
Habitat.dataset = 'habitat_test_castle'
# Habitat.dataset = 'habitat_test_single'
Habitat.gpu_id = 0
Habitat.image_key = 'rgb'
Habitat.goal_key = 'pointgoal_with_gps_compass'

# Reward
dense_reward.slack_reward = -0.01
dense_reward.success_reward = 10.0
dense_reward.distance_scaling = 1.0
optimal_path_length_reward.scaling = 1.0
collision_penalty.scaling = 1.0
obstacle_distance_penalty.threshold = 0.5
obstacle_distance_penalty.scaling = 1.0
Habitat.reward_function = @combine_rewards()
combine_rewards.rewards = [@dense_reward(), @collision_penalty(), @obstacle_distance_penalty()]
# combine_rewards.rewards = [@optimal_path_length_reward(), @collision_penalty(), @obstacle_distance_penalty()]
