main.base_logdir = 'logs'
logging.module_levels = {'matplotlib': 'WARNING', 'PIL': 'WARNING', 'imageio_ffmpeg': 'ERROR'}

### Planet ###
# Task
planet.tasks = ['habitat']
planet.isolate_envs = 'none'  # Don't change
planet.max_task_length = 150  # Maximum sequence length
planet.gradient_heads = ['image', 'goal', 'reward']
planet.train_action_noise = 0.3

# Schedule
planet.num_seed_episodes = 20
planet.train_collect_every = 1  # epochs per training data collection
planet.num_train_episodes = 5  # training episodes per data collection
planet.test_collect_every = 2  # epochs per test data collection
planet.num_test_episodes = 10  # episodes per test data collection (these are also used for evaluation)
planet.train_steps = 100  # training batches per epoch
planet.test_steps = 1  # test steps per epoch
planet.log_summaries_every = 3  # epochs between summaries
planet.max_epochs = 200  # maximum number of epochs
planet.checkpoint_every = 5  # epochs between checkpoints
planet.eval_every = 20  # epochs between evaluations
train.num_eval_episodes = 10  # episodes per evaluation
planet.checkpoint_to_load = None  # can be a dir or a specific file, absolute or relative to the logdir

# Example: continue from still-gorge-205
# planet.checkpoint_to_load = '../20191010-114731/00001'
# planet.checkpoint_load_exclude = [r'.*collect_.*_main_habitat.*']  # For compatibility with old checkpoints
# planet.max_epochs = 20000

# Losses
planet.divergence_scale = 1.0
planet.image_loss_scale = 1.0
planet.reward_loss_scale = 10.0
planet.goal_loss_scale = 10.0

# Model complexity
planet.model_size = 200  # belief_size, embed_size
planet.state_size = 30
planet.num_layers = 1  # used for reward and goal prediction
planet.num_units = 100  # used for reward and goal prediction
planet.batch_shape = [64, 10]  # [batch_size, min_seq_length]

# Planner
planet.planner = 'cem'
planet.planner_horizon = 12
planet.planner_iterations = 10
planet.planner_amount = 1000  # number of action sequence samples per iteration
planet.planner_topk = 100  # number of best samples to use as basis for next iteration

# TF options
planet.tf.options.log_device_placement = False
# planet.tf.options.device_count = {'GPU': 0, 'CPU': 4}
planet.tf.debugger = False

# GPU options
planet.tf.gpu_options.allow_growth = True
planet.tf.gpu_options.visible_device_list = '1'
# planet.tf.gpu_options.per_process_gpu_memory_fraction = 0.5

### Habitat ###
# Basic
planet_habitat_task.wrappers = [@curriculum(),
                                @action_repeat()]
curriculum.enabled = True
curriculum.start_threshold = 1.5  # Initial threshold in meters.
curriculum.initial_delay = 200  # Number of episodes to wait before increasing threshold.
curriculum.increase_rate = 0.05  # Rate of increase in meters per episode.
Habitat.task = 'pointnav'
Habitat.dataset = 'habitat_test'
Habitat.gpu_id = 0
Habitat.image_key = 'rgb'
Habitat.goal_key = 'pointgoal_with_gps_compass'

# Reward
dense_reward.slack_reward = -0.01
dense_reward.success_reward = 10.0
dense_reward.distance_scaling = 1.0
optimal_path_length_reward.scaling = 1.0
collision_penalty.scaling = 1.0
obstacle_distance_penalty.threshold = 0.5
obstacle_distance_penalty.scaling = 1.0
Habitat.reward_function = @combine_rewards()
combine_rewards.rewards = [@dense_reward(), @collision_penalty(), @obstacle_distance_penalty()]
