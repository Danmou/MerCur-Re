main.base_logdir = 'logs'
logging.module_levels = {'matplotlib': 'WARNING', 'PIL': 'WARNING', 'imageio_ffmpeg': 'ERROR', 'ffmpeg': 'ERROR',
                         'urllib3': 'INFO', 'numba': 'INFO'}

### Planet ###
# Task
training.tasks = [@habitat_train_task()]
evaluation.tasks = [@habitat_eval_task()]

# Data preprocessing
numpy_episodes.train_action_noise = 0.3
numpy_episodes.success_padding = 32  # Number of padding elements equal to the last element to add after successful episodes

# Schedule
training.num_seed_episodes = 40
DataCollectionCallback.period = 1  # epochs per data collection
DataCollectionCallback.train_episodes = 4  # training episodes per data collection
DataCollectionCallback.test_episodes = 2  # episodes per test data collection
training.train_steps = 100  # training batches per epoch
training.test_steps = 10  # test steps per epoch
training.num_epochs = 1e4
CheckpointCallback.period = 5  # epochs between checkpoints
EvaluateCallback.period = 10  # epochs between evaluations
EvaluateCallback.num_episodes = 10  # episodes per evaluation
PredictionSummariesCallback.period = 5
LoggingCallback.epoch_log_period = 1
LoggingCallback.epoch_header_period = 5  # how many times to log before reprinting headers
LoggingCallback.batch_log_period = 10
LoggingCallback.batch_header_period = 10  # how many times to log before reprinting headers

# Losses
SimpleRNN.divergence_loss_free_nats = 3.0
SimpleRNN.divergence_loss_scale = 0.1
HierarchicalRNN.divergence_loss_free_nats = 3.0
HierarchicalRNN.divergence_loss_scales = [0.06, 0.03, 0.01]
reconstruction_loss.scales = {
    'image': 600.0,
    'reward': 0.5,
    'goal': 1.0,
    'done': 0.5,
}
compute_value_loss.lambda_ = 0.95
compute_action_return.lambda_ = 0.95

# Model complexity
Model.predictor_class = @predictors.RSSMPredictor
predictors.OpenLoopRSSMPredictor.state_size = 40
predictors.OpenLoopRSSMPredictor.belief_size = 300
predictors.OpenLoopRSSMPredictor.embed_size = 300
predictors.OpenLoopRSSMPredictor.num_layers = 1
predictors.OpenLoopRSSMPredictor.activation = @auto_shape.ReLU
# Model.rnn_class = @rnns.SimpleRNN
Model.rnn_class = @rnns.HierarchicalRNN
rnns.HierarchicalRNN.open_loop_predictor_class = @predictors.OpenLoopRSSMPredictor
rnns.HierarchicalRNN.time_scales = [1, 3, 9]
rnns.HierarchicalRNN.action_embedding_sizes = [1, 2, 2]
DenseVAE.num_units = 100
DenseVAE.num_layers = 1
DenseVAE.activation = @auto_shape.ReLU
Model.decoders.num_layers = 2  # used for reward, done and goal prediction
Model.decoders.num_units = 400  # used for reward, done and goal prediction
Model.decoders.activation = @auto_shape.ReLU
Encoder.activation = @auto_shape.ReLU
Decoder.activation = @auto_shape.ReLU
Model.action_network.num_layers = 4
Model.action_network.num_units = 400
Model.action_network.activation = @auto_shape.ReLU
Model.value_network.num_layers = 3
Model.value_network.num_units = 400
Model.value_network.activation = @auto_shape.ReLU
training.batch_shape = (64, 64)  # [batch_size, time_batch]
Model.dreamer = True
imagine_forward.horizon = 15

# Optimizer
get_model.optimizers = {'model': @tf.keras.optimizers.Adam(),
                        'action_network': @action/tf.keras.optimizers.Adam(),
                        'value_network': @value/tf.keras.optimizers.Adam()}
tf.keras.optimizers.Adam.learning_rate = 6e-4
tf.keras.optimizers.Adam.epsilon = 1e-4
action/tf.keras.optimizers.Adam.learning_rate = 8e-5
value/tf.keras.optimizers.Adam.learning_rate = 8e-5
run_on_batch.gradient_clip_norm = 100.0

# Agent and planner
training.agent_cls = @train/PolicyNetworkAgent
get_evaluation_agent.agent_cls = @eval/PolicyNetworkAgent  # If None, the training agent is reused
train/PolicyNetworkAgent.sample = True
eval/PolicyNetworkAgent.sample = False
MPCAgent.objective = 'reward'
MPCAgent.planner = @CrossEntropyMethod
CrossEntropyMethod.horizon = 12
CrossEntropyMethod.iterations = 10
CrossEntropyMethod.amount = 1000  # number of action sequence samples per iteration
CrossEntropyMethod.top_k = 100  # number of best samples to use as basis for next iteration
HierarchicalCrossEntropyMethod.horizon = 10
HierarchicalCrossEntropyMethod.iterations = 4
HierarchicalCrossEntropyMethod.amount = 1000  # number of action sequence samples per iteration
HierarchicalCrossEntropyMethod.top_k = 100  # number of best samples to use as basis for next iteration

# TF options
# tf.options.log_device_placement = False
# tf.options.device_count = {'GPU': 0, 'CPU': 4}
# tf.debugger = False

# GPU options
tf.gpus.memory_growth = True
tf.gpus.gpu_ids = [0]
# tf.gpu_options.per_process_gpu_memory_fraction = 0.5

### Habitat ###
# Basic
habitat_train_task.max_length = 150  # Maximum sequence length
habitat_train_task.wrappers = [@curriculum(), @action_repeat()]
habitat_eval_task.max_length = 150   # Maximum sequence length
habitat_eval_task.wrappers = [@action_repeat()]
curriculum.enabled = False
curriculum.start_threshold = 1.5  # Initial threshold in meters.
curriculum.initial_delay = 200  # Number of episodes to wait before increasing threshold.
curriculum.increase_rate = 0.05  # Rate of increase in meters per episode.
CollectGymDataset.rejection_metric = None
Habitat.task = 'pointnav'
Habitat.train_dataset = 'gibson'
Habitat.eval_dataset = 'gibson'
Habitat.train_split = 'train'
Habitat.eval_split = 'val_mini'
Habitat.gpu_id = 0
Habitat.image_key = 'rgb'
Habitat.goal_key = 'pointgoal_with_gps_compass'

# Reward
dense_reward.slack_reward = -0.01
dense_reward.success_reward = 10.0
dense_reward.distance_scaling = 1.0
optimal_path_length_reward.scaling = 1.0
collision_penalty.scaling = 1.0
obstacle_distance_penalty.threshold = 0.5
obstacle_distance_penalty.scaling = 1.0
Habitat.reward_function = @combine_rewards()
combine_rewards.rewards = [@dense_reward(), @collision_penalty(), @obstacle_distance_penalty()]
